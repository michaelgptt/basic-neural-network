{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "216c2403-dafe-4e8d-9731-1de23cbd860e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far finished  0  epoch\n",
      "So far finished  1  epoch\n",
      "So far finished  2  epoch\n",
      "So far finished  3  epoch\n",
      "So far finished  4  epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tvm159\\AppData\\Local\\Temp\\4\\ipykernel_17360\\3838046252.py:60: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-colVector))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far finished  5  epoch\n",
      "So far finished  6  epoch\n",
      "So far finished  7  epoch\n",
      "So far finished  8  epoch\n",
      "So far finished  9  epoch\n",
      "[[0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.19307301e-239]\n",
      " [2.40548937e-212]\n",
      " [2.19623755e-017]\n",
      " [2.89709141e-128]\n",
      " [1.14448458e-113]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.71345921e-203]]\n",
      "4\n",
      "[[0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.19307301e-239]\n",
      " [2.40548937e-212]\n",
      " [2.19623755e-017]\n",
      " [2.89709141e-128]\n",
      " [1.14448458e-113]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.71345921e-203]]\n",
      "4\n",
      "[[0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.19307301e-239]\n",
      " [2.40548937e-212]\n",
      " [2.19623755e-017]\n",
      " [2.89709141e-128]\n",
      " [1.14448458e-113]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.71345921e-203]]\n",
      "4\n",
      "[[0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.19307301e-239]\n",
      " [2.40548937e-212]\n",
      " [2.19623755e-017]\n",
      " [2.89709141e-128]\n",
      " [1.14448458e-113]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.71345921e-203]]\n",
      "4\n",
      "[[0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.19307301e-239]\n",
      " [2.40548937e-212]\n",
      " [2.19623755e-017]\n",
      " [2.89709141e-128]\n",
      " [1.14448458e-113]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.71345921e-203]]\n",
      "4\n",
      "[[0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.19307301e-239]\n",
      " [2.40548937e-212]\n",
      " [2.19623755e-017]\n",
      " [2.89709141e-128]\n",
      " [1.14448458e-113]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.71345921e-203]]\n",
      "4\n",
      "[[0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.19307301e-239]\n",
      " [2.40548937e-212]\n",
      " [2.19623755e-017]\n",
      " [2.89709141e-128]\n",
      " [1.14448458e-113]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.71345921e-203]]\n",
      "4\n",
      "[[0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.19307301e-239]\n",
      " [2.40548937e-212]\n",
      " [2.19623755e-017]\n",
      " [2.89709141e-128]\n",
      " [1.14448458e-113]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.71345921e-203]]\n",
      "4\n",
      "[[0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.19307301e-239]\n",
      " [2.40548937e-212]\n",
      " [2.19623755e-017]\n",
      " [2.89709141e-128]\n",
      " [1.14448458e-113]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.71345921e-203]]\n",
      "4\n",
      "[[0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.19307301e-239]\n",
      " [2.40548937e-212]\n",
      " [2.19623755e-017]\n",
      " [2.89709141e-128]\n",
      " [1.14448458e-113]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [2.71345921e-203]]\n",
      "4\n",
      "Accuracy of Model: 20.0 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#IN ORDER TO RUN TRAINING, you'll need to download the .csv version of the MNIST dataset from \n",
    "#https://www.kaggle.com/datasets/oddrationale/mnist-in-csv and place mnist_train.csv and mnist_test.csv\n",
    "#in the mnist-dataset directory.\n",
    "\n",
    "class Neural_Network:\n",
    "    \n",
    "    def __init__(self, epochs, learningRate, batchSize):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.learningRate = learningRate\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "        #Reads in MNIST training set\n",
    "        self.data_initial = pd.read_csv('./mnist-dataset/mnist_train.csv')\n",
    "        self.labels = (self.data_initial['label']).to_numpy().reshape(60000,1) #(60000 imgs,)\n",
    "        self.data = (self.data_initial.drop('label', axis=1)).to_numpy().reshape(60000,784) #(60000 imgs, 784, pixels)\n",
    "        self.trainingSetSize = self.labels.size\n",
    "        \n",
    "        #Reads in MNIST test set\n",
    "        self.testInitial = pd.read_csv('./mnist-dataset/mnist_test.csv')\n",
    "        self.testLabels = (self.testInitial['label']).to_numpy().reshape(10000,1) #(10000 imgs,1)\n",
    "        self.testData = (self.testInitial.drop('label', axis=1)).to_numpy().reshape(10000,784) #(10000 imgs, 784, pixels)\n",
    "        self.testSetSize = self.testLabels.size\n",
    "\n",
    "        self.a_0 = np.zeros([784, 1]) #np.empty([10,1])\n",
    "\n",
    "        self.W_1 = np.random.default_rng().normal(loc=0, scale=(1/np.sqrt(16)), size=(16,784)) #Randomly intialized weight matrix\n",
    "        self.b_1 = np.random.default_rng().normal(loc=0, scale=1, size=(16,1)) #Randomly intialized bias column vector\n",
    "        self.z_1 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        self.a_1 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        self.error_1 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        \n",
    "        self.W_2 = np.random.default_rng().normal(loc=0, scale=(1/np.sqrt(16)), size=(16,16))\n",
    "        self.b_2 = np.random.default_rng().normal(loc=0, scale=1, size=(16,1))\n",
    "        self.z_2 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        self.a_2 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        self.error_2 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "       \n",
    "        self.W_3 = np.random.default_rng().normal(loc=0, scale=(1/np.sqrt(10)), size=(10,16))\n",
    "        self.b_3 = np.random.default_rng().normal(loc=0, scale=1, size=(10,1))\n",
    "        self.z_3 = np.zeros([10, 1]) #np.empty([10, 1])\n",
    "        self.a_3 = np.zeros([10, 1]) #np.empty([10, 1])\n",
    "        self.error_out = np.zeros([10, 1]) #np.empty([10, 1])\n",
    "\n",
    "        self.dW_1 = np.zeros([16, 784]) # np.empty([16, 784])\n",
    "        self.dB_1 = np.zeros([16,1]) #np.empty([16,1])\n",
    "        self.dW_2 = np.zeros([16,16]) #np.empty([16,16])\n",
    "        self.dB_2 = np.zeros([16,1]) #np.empty([16,1])\n",
    "        self.dW_3 = np.zeros([10,16]) #np.empty([10,16])\n",
    "        self.dB_3 = np.zeros([10,1]) #np.empty([10,1])\n",
    "        \n",
    "        #Lesson: It seems np.empty can cause many errors.\n",
    "\n",
    "        self.y = np.zeros([10,1]).astype(int)\n",
    "\n",
    "    def sigmoid(self, colVector):\n",
    "        return 1/(1 + np.exp(-colVector))\n",
    "\n",
    "    def dSigmoid(self, colVector):\n",
    "        return (self.sigmoid(colVector)) * (1 - self.sigmoid(colVector))\n",
    "\n",
    "    def feedForward(self, x, dataset):\n",
    "        #Calculates all the activations in the network for the training example, x.\n",
    "\n",
    "        #Grabs image pixel information from the xth row of the dataset. \n",
    "        #This gives us a numpy (784,1) colm vector of activations for a training example, x, \n",
    "        #on Layer 0 (input layer)\n",
    "\n",
    "        #1 means feedForward a training example from the training dataset\n",
    "        if dataset == \"training\":\n",
    "            self.a_0 = self.sigmoid(self.data[x, :].reshape(784,1)) #MAKE SURE YOU SQUISH VALUES DOWN\n",
    "\n",
    "        #2 means feedForward a training example from the test dataset.\n",
    "        if dataset == \"testing\":\n",
    "            self.a_0 = self.sigmoid(self.testData[x, :].reshape(784,1)) #MAKE SURE YOU SQUISH VALUES DOWN\n",
    "\n",
    "        #Going into Layer 1\n",
    "        self.z_1 = (np.dot(self.W_1, self.a_0)) + self.b_1\n",
    "        self.a_1 = self.sigmoid(self.z_1)\n",
    "\n",
    "        #Going into Layer 2\n",
    "        self.z_2 = (np.dot(self.W_2, self.a_1)) + self.b_2\n",
    "        self.a_2 = self.sigmoid(self.z_2)\n",
    "\n",
    "        #Going into Layer 3 (output layer)\n",
    "        self.z_3 = (np.dot(self.W_3, self.a_2)) + self.b_3\n",
    "        self.a_3 = self.sigmoid(self.z_3)\n",
    "\n",
    "    def backProp(self, x): \n",
    "        #Calculates the \"error\" on all the neurons in the network for a training example, x.\n",
    "        \n",
    "        #Creates y column vector that represents the ideal output for all the output neurons for the spesific training example.\n",
    "        self.y[self.labels[x, 0], 0] = 1\n",
    "        #print(self.y)\n",
    "\n",
    "        #Calculuate the error on the output neurons\n",
    "        self.error_out = (self.a_3 - self.y) * self.dSigmoid(self.z_3)\n",
    "\n",
    "        #Calculate the error on each of neurons on each of the layers. Calculating backwards.\n",
    "        self.error_2 = np.dot((np.transpose(self.W_3)), self.error_out) * self.dSigmoid(self.z_2)\n",
    "        self.error_1 = np.dot((np.transpose(self.W_2)), self.error_2) * self.dSigmoid(self.z_1)\n",
    "\n",
    "        self.y = np.zeros([10,1]).astype(int)\n",
    "\n",
    "    def accumulateGradients(self):\n",
    "        #Calculates the derivative of the cost function WRT all the weights and biases. <-- Gradient information\n",
    "        #\"Accumulating\" (i.e. adding together) the graidents (element wise) of each of the training examples that go through.\n",
    "\n",
    "        self.dW_1 = self.dW_1 + (np.dot(self.error_1, np.transpose(self.a_0))) #(16,1) dot (1,784) = (16,784)\n",
    "        self.dB_1 = self.dB_1 + self.error_1 #(16,1)\n",
    "        \n",
    "        self.dW_2 = self.dW_2 + (np.dot(self.error_2, np.transpose(self.a_1))) #(16,1) dot (1,16) = (16,16)\n",
    "        self.dB_2 = self.dB_2 + self.error_2 #(16,1)\n",
    "\n",
    "        self.dW_3 = self.dW_3 + (np.dot(self.error_out, np.transpose(self.a_2))) #(10,1) dot (1,16) = (10,16)\n",
    "        self.dB_3 = self.dB_3 + self.error_out #(10,1)\n",
    "\n",
    "    def applyAvgGradient(self):\n",
    "        n = self.learningRate\n",
    "        m = self.batchSize\n",
    "        \n",
    "        self.W_1 = self.W_1 - ((n/m)*self.dW_1)\n",
    "        self.b_1 = self.b_1 - ((n/m)*self.dB_1)\n",
    "\n",
    "        self.W_2 = self.W_2 - ((n/m)*self.dW_2)\n",
    "        self.b_2 = self.b_2 - ((n/m)*self.dB_2)\n",
    "\n",
    "        self.W_3 = self.W_3 - ((n/m)*self.dW_3)\n",
    "        self.b_3 = self.b_3 - ((n/m)*self.dB_3)\n",
    "\n",
    "    def startTraining(self):\n",
    "        for epochs in range(self.epochs):\n",
    "\n",
    "            #60,000 examples (x), I want 1,000 examples per batch = 60 batch\n",
    "            for batch in range(int(self.trainingSetSize/self.batchSize)): #int(self.trainingSetSize/self.batchSize)\n",
    "                for x in range(self.batchSize):  #self.batchSize #1,000 per batch\n",
    "                    self.feedForward(x, \"training\")\n",
    "                    self.backProp(x)\n",
    "                    self.accumulateGradients()\n",
    "                self.applyAvgGradient() \n",
    "                #It seems the moment we apply the avgGradient, ALLL WEIGHT VALUES BECOME NAN\n",
    "\n",
    "            print(\"So far finished \", epochs, \" epoch\")\n",
    "     \n",
    "    def evaluate(self):\n",
    "        #Evaluates the models accuracy by running through the 10,000 test examples and seeing how many test examples the model gets right.\n",
    "        correct = 0\n",
    "        for x in range(10):\n",
    "            \n",
    "            self.feedForward(x, \"testing\")\n",
    "            print(self.a_3)\n",
    "            print(np.argmax(self.a_3))\n",
    "            \n",
    "            if (np.argmax(self.a_3) == self.testLabels[x, 0]):\n",
    "                correct += 1 \n",
    "        return (correct/10)*100 \n",
    "\n",
    "#12960 weights, 42 biases\n",
    "nn = Neural_Network(10, 1, 1000)\n",
    "nn.startTraining()\n",
    "print(\"Accuracy of Model:\", nn.evaluate(), \"%\")\n",
    "\n",
    "#Unknown Error Hypothesis:\n",
    "#For some reason, the model seems to refuse to update it's weights and biases, causing patterns in data to not be learned even after many epochs.\n",
    "#The activations on the last layer are changing slightly everytime we input a different test example, indicating we are recomputing a_3 for every x.\n",
    "#However, the activations in the last layer are so similar that it seems to me that the weights and biases are learning VERY VERY slowly.\n",
    "\n",
    "#The follwoing was the result after a high learning rate (1) and 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70821f11-ae99-4777-9506-4a43e7258db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far finished  0  epoch\n",
      "So far finished  1  epoch\n",
      "So far finished  2  epoch\n",
      "So far finished  3  epoch\n",
      "So far finished  4  epoch\n",
      "So far finished  5  epoch\n",
      "So far finished  6  epoch\n",
      "So far finished  7  epoch\n",
      "So far finished  8  epoch\n",
      "So far finished  9  epoch\n",
      "[[6.72939684e-058]\n",
      " [2.43840275e-063]\n",
      " [8.43723714e-002]\n",
      " [2.15058372e-034]\n",
      " [1.66516624e-081]\n",
      " [1.60390517e-061]\n",
      " [1.22548510e-093]\n",
      " [2.40977952e-075]\n",
      " [2.53108341e-116]\n",
      " [1.27060735e-002]]\n",
      "2\n",
      "[[6.67180152e-058]\n",
      " [2.41491560e-063]\n",
      " [8.47717868e-002]\n",
      " [2.13949193e-034]\n",
      " [1.64491116e-081]\n",
      " [1.58793087e-061]\n",
      " [1.20894430e-093]\n",
      " [2.38125416e-075]\n",
      " [2.48602769e-116]\n",
      " [1.25883364e-002]]\n",
      "2\n",
      "[[6.82721753e-058]\n",
      " [2.47653009e-063]\n",
      " [8.40352716e-002]\n",
      " [2.16819737e-034]\n",
      " [1.69950912e-081]\n",
      " [1.62901683e-061]\n",
      " [1.25372100e-093]\n",
      " [2.45609614e-075]\n",
      " [2.60533921e-116]\n",
      " [1.28178487e-002]]\n",
      "2\n",
      "[[6.67214404e-058]\n",
      " [2.41506161e-063]\n",
      " [8.47680218e-002]\n",
      " [2.13956236e-034]\n",
      " [1.64503186e-081]\n",
      " [1.58803379e-061]\n",
      " [1.20904218e-093]\n",
      " [2.38143221e-075]\n",
      " [2.48630595e-116]\n",
      " [1.25893959e-002]]\n",
      "2\n",
      "[[6.68671774e-058]\n",
      " [2.42091295e-063]\n",
      " [8.46813695e-002]\n",
      " [2.14232345e-034]\n",
      " [1.65013156e-081]\n",
      " [1.59197619e-061]\n",
      " [1.21321004e-093]\n",
      " [2.38853060e-075]\n",
      " [2.49750590e-116]\n",
      " [1.26152906e-002]]\n",
      "2\n",
      "[[7.19666839e-058]\n",
      " [2.62365113e-063]\n",
      " [8.24505905e-002]\n",
      " [2.23481490e-034]\n",
      " [1.83206795e-081]\n",
      " [1.72714333e-061]\n",
      " [1.36318484e-093]\n",
      " [2.63634086e-075]\n",
      " [2.90151624e-116]\n",
      " [1.33384185e-002]]\n",
      "2\n",
      "[[6.99565357e-058]\n",
      " [2.54459799e-063]\n",
      " [8.31226647e-002]\n",
      " [2.19941073e-034]\n",
      " [1.75972687e-081]\n",
      " [1.67484732e-061]\n",
      " [1.30325750e-093]\n",
      " [2.53922747e-075]\n",
      " [2.74025170e-116]\n",
      " [1.31041513e-002]]\n",
      "2\n",
      "[[4.95081387e-055]\n",
      " [3.57628846e-060]\n",
      " [3.58003771e-003]\n",
      " [7.25046906e-033]\n",
      " [3.13237210e-077]\n",
      " [4.17605854e-058]\n",
      " [6.89701233e-089]\n",
      " [2.10611346e-071]\n",
      " [4.30805940e-110]\n",
      " [9.03876379e-001]]\n",
      "9\n",
      "[[6.70296257e-058]\n",
      " [2.42718786e-063]\n",
      " [8.46312749e-002]\n",
      " [2.14524385e-034]\n",
      " [1.65578786e-081]\n",
      " [1.59608779e-061]\n",
      " [1.21785336e-093]\n",
      " [2.39613001e-075]\n",
      " [2.50958042e-116]\n",
      " [1.26320663e-002]]\n",
      "2\n",
      "[[8.54679178e-055]\n",
      " [6.57020402e-060]\n",
      " [2.79877297e-003]\n",
      " [9.80085231e-033]\n",
      " [7.01351390e-077]\n",
      " [7.90335161e-058]\n",
      " [1.69271882e-088]\n",
      " [4.45722107e-071]\n",
      " [1.38707027e-109]\n",
      " [9.38951124e-001]]\n",
      "9\n",
      "Accuracy of Model: 30.0 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#IN ORDER TO RUN TRAINING, you'll need to download the .csv version of the MNIST dataset from \n",
    "#https://www.kaggle.com/datasets/oddrationale/mnist-in-csv and place mnist_train.csv and mnist_test.csv\n",
    "#in the mnist-dataset directory.\n",
    "\n",
    "class Neural_Network:\n",
    "    \n",
    "    def __init__(self, epochs, learningRate, batchSize):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.learningRate = learningRate\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "        #Reads in MNIST training set\n",
    "        self.data_initial = pd.read_csv('./mnist-dataset/mnist_train.csv')\n",
    "        self.labels = (self.data_initial['label']).to_numpy().reshape(60000,1) #(60000 imgs,)\n",
    "        self.data = (self.data_initial.drop('label', axis=1)).to_numpy().reshape(60000,784) #(60000 imgs, 784, pixels)\n",
    "        self.trainingSetSize = self.labels.size\n",
    "        \n",
    "        #Reads in MNIST test set\n",
    "        self.testInitial = pd.read_csv('./mnist-dataset/mnist_test.csv')\n",
    "        self.testLabels = (self.testInitial['label']).to_numpy().reshape(10000,1) #(10000 imgs,1)\n",
    "        self.testData = (self.testInitial.drop('label', axis=1)).to_numpy().reshape(10000,784) #(10000 imgs, 784, pixels)\n",
    "        self.testSetSize = self.testLabels.size\n",
    "\n",
    "        self.a_0 = np.zeros([784, 1]) #np.empty([10,1])\n",
    "\n",
    "        self.W_1 = np.random.default_rng().normal(loc=0, scale=(1/np.sqrt(16)), size=(16,784)) #Randomly intialized weight matrix\n",
    "        self.b_1 = np.random.default_rng().normal(loc=0, scale=1, size=(16,1)) #Randomly intialized bias column vector\n",
    "        self.z_1 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        self.a_1 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        self.error_1 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        \n",
    "        self.W_2 = np.random.default_rng().normal(loc=0, scale=(1/np.sqrt(16)), size=(16,16))\n",
    "        self.b_2 = np.random.default_rng().normal(loc=0, scale=1, size=(16,1))\n",
    "        self.z_2 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        self.a_2 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        self.error_2 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "       \n",
    "        self.W_3 = np.random.default_rng().normal(loc=0, scale=(1/np.sqrt(10)), size=(10,16))\n",
    "        self.b_3 = np.random.default_rng().normal(loc=0, scale=1, size=(10,1))\n",
    "        self.z_3 = np.zeros([10, 1]) #np.empty([10, 1])\n",
    "        self.a_3 = np.zeros([10, 1]) #np.empty([10, 1])\n",
    "        self.error_out = np.zeros([10, 1]) #np.empty([10, 1])\n",
    "\n",
    "        self.dW_1 = np.zeros([16, 784]) # np.empty([16, 784])\n",
    "        self.dB_1 = np.zeros([16,1]) #np.empty([16,1])\n",
    "        self.dW_2 = np.zeros([16,16]) #np.empty([16,16])\n",
    "        self.dB_2 = np.zeros([16,1]) #np.empty([16,1])\n",
    "        self.dW_3 = np.zeros([10,16]) #np.empty([10,16])\n",
    "        self.dB_3 = np.zeros([10,1]) #np.empty([10,1])\n",
    "        \n",
    "        #Lesson: It seems np.empty can cause many errors.\n",
    "\n",
    "        self.y = np.zeros([10,1]).astype(int)\n",
    "\n",
    "    def sigmoid(self, colVector):\n",
    "        return 1/(1 + np.exp(-colVector))\n",
    "\n",
    "    def dSigmoid(self, colVector):\n",
    "        return (self.sigmoid(colVector)) * (1 - self.sigmoid(colVector))\n",
    "\n",
    "    def feedForward(self, x, dataset):\n",
    "        #Calculates all the activations in the network for the training example, x.\n",
    "\n",
    "        #Grabs image pixel information from the xth row of the dataset. \n",
    "        #This gives us a numpy (784,1) colm vector of activations for a training example, x, \n",
    "        #on Layer 0 (input layer)\n",
    "\n",
    "        #1 means feedForward a training example from the training dataset\n",
    "        if dataset == \"training\":\n",
    "            self.a_0 = self.sigmoid(self.data[x, :].reshape(784,1)) #MAKE SURE YOU SQUISH VALUES DOWN\n",
    "\n",
    "        #2 means feedForward a training example from the test dataset.\n",
    "        if dataset == \"testing\":\n",
    "            self.a_0 = self.sigmoid(self.testData[x, :].reshape(784,1)) #MAKE SURE YOU SQUISH VALUES DOWN\n",
    "\n",
    "        #Going into Layer 1\n",
    "        self.z_1 = (np.dot(self.W_1, self.a_0)) + self.b_1\n",
    "        self.a_1 = self.sigmoid(self.z_1)\n",
    "\n",
    "        #Going into Layer 2\n",
    "        self.z_2 = (np.dot(self.W_2, self.a_1)) + self.b_2\n",
    "        self.a_2 = self.sigmoid(self.z_2)\n",
    "\n",
    "        #Going into Layer 3 (output layer)\n",
    "        self.z_3 = (np.dot(self.W_3, self.a_2)) + self.b_3\n",
    "        self.a_3 = self.sigmoid(self.z_3)\n",
    "\n",
    "    def backProp(self, x): \n",
    "        #Calculates the \"error\" on all the neurons in the network for a training example, x.\n",
    "        \n",
    "        #Creates y column vector that represents the ideal output for all the output neurons for the spesific training example.\n",
    "        self.y[self.labels[x, 0], 0] = 1\n",
    "        #print(self.y)\n",
    "\n",
    "        #Calculuate the error on the output neurons\n",
    "        self.error_out = (self.a_3 - self.y) * self.dSigmoid(self.z_3)\n",
    "\n",
    "        #Calculate the error on each of neurons on each of the layers. Calculating backwards.\n",
    "        self.error_2 = np.dot((np.transpose(self.W_3)), self.error_out) * self.dSigmoid(self.z_2)\n",
    "        self.error_1 = np.dot((np.transpose(self.W_2)), self.error_2) * self.dSigmoid(self.z_1)\n",
    "\n",
    "        self.y = np.zeros([10,1]).astype(int)\n",
    "\n",
    "    def accumulateGradients(self):\n",
    "        #Calculates the derivative of the cost function WRT all the weights and biases. <-- Gradient information\n",
    "        #\"Accumulating\" (i.e. adding together) the graidents (element wise) of each of the training examples that go through.\n",
    "\n",
    "        self.dW_1 = self.dW_1 + (np.dot(self.error_1, np.transpose(self.a_0))) #(16,1) dot (1,784) = (16,784)\n",
    "        self.dB_1 = self.dB_1 + self.error_1 #(16,1)\n",
    "        \n",
    "        self.dW_2 = self.dW_2 + (np.dot(self.error_2, np.transpose(self.a_1))) #(16,1) dot (1,16) = (16,16)\n",
    "        self.dB_2 = self.dB_2 + self.error_2 #(16,1)\n",
    "\n",
    "        self.dW_3 = self.dW_3 + (np.dot(self.error_out, np.transpose(self.a_2))) #(10,1) dot (1,16) = (10,16)\n",
    "        self.dB_3 = self.dB_3 + self.error_out #(10,1)\n",
    "\n",
    "    def applyAvgGradient(self):\n",
    "        n = self.learningRate\n",
    "        m = self.batchSize\n",
    "        \n",
    "        self.W_1 = self.W_1 - ((n/m)*self.dW_1)\n",
    "        self.b_1 = self.b_1 - ((n/m)*self.dB_1)\n",
    "\n",
    "        self.W_2 = self.W_2 - ((n/m)*self.dW_2)\n",
    "        self.b_2 = self.b_2 - ((n/m)*self.dB_2)\n",
    "\n",
    "        self.W_3 = self.W_3 - ((n/m)*self.dW_3)\n",
    "        self.b_3 = self.b_3 - ((n/m)*self.dB_3)\n",
    "\n",
    "    def startTraining(self):\n",
    "        for epochs in range(self.epochs):\n",
    "\n",
    "            #60,000 examples (x), I want 1,000 examples per batch = 60 batch\n",
    "            for batch in range(int(self.trainingSetSize/self.batchSize)): #int(self.trainingSetSize/self.batchSize)\n",
    "                for x in range(self.batchSize):  #self.batchSize #1,000 per batch\n",
    "                    self.feedForward(x, \"training\")\n",
    "                    self.backProp(x)\n",
    "                    self.accumulateGradients()\n",
    "                self.applyAvgGradient() \n",
    "                #It seems the moment we apply the avgGradient, ALLL WEIGHT VALUES BECOME NAN\n",
    "\n",
    "            print(\"So far finished \", epochs, \" epoch\")\n",
    "     \n",
    "    def evaluate(self):\n",
    "        #Evaluates the models accuracy by running through the 10,000 test examples and seeing how many test examples the model gets right.\n",
    "        correct = 0\n",
    "        for x in range(10):\n",
    "            \n",
    "            self.feedForward(x, \"testing\")\n",
    "            print(self.a_3)\n",
    "            print(np.argmax(self.a_3))\n",
    "            \n",
    "            if (np.argmax(self.a_3) == self.testLabels[x, 0]):\n",
    "                correct += 1 \n",
    "        return (correct/10)*100 \n",
    "\n",
    "#12960 weights, 42 biases\n",
    "nn = Neural_Network(10, 0.1, 1000)\n",
    "nn.startTraining()\n",
    "print(\"Accuracy of Model:\", nn.evaluate(), \"%\")\n",
    "\n",
    "#Unknown Error Hypothesis:\n",
    "#For some reason, the model seems to refuse to update it's weights and biases, causing patterns in data to not be learned even after many epochs.\n",
    "#The activations on the last layer are changing slightly everytime we input a different test example, indicating we are recomputing a_3 for every x.\n",
    "#However, the activations in the last layer are so similar that it seems to me that the weights and biases are learning VERY VERY slowly.\n",
    "\n",
    "#The following was after a low learning rate (0.1) and 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70aff023-c571-4385-b828-f6519699d391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far finished  0  epoch\n",
      "So far finished  1  epoch\n",
      "So far finished  2  epoch\n",
      "So far finished  3  epoch\n",
      "So far finished  4  epoch\n",
      "So far finished  5  epoch\n",
      "So far finished  6  epoch\n",
      "So far finished  7  epoch\n",
      "So far finished  8  epoch\n",
      "So far finished  9  epoch\n",
      "[[3.26510901e-142]\n",
      " [3.56055372e-040]\n",
      " [2.51776361e-004]\n",
      " [1.44883288e-001]\n",
      " [1.57249556e-117]\n",
      " [5.83704810e-112]\n",
      " [2.00947146e-083]\n",
      " [3.10546362e-151]\n",
      " [3.54581722e-025]\n",
      " [7.58686150e-047]]\n",
      "3\n",
      "[[3.26485527e-142]\n",
      " [3.56048371e-040]\n",
      " [2.51776418e-004]\n",
      " [1.44883374e-001]\n",
      " [1.57239574e-117]\n",
      " [5.83670709e-112]\n",
      " [2.00938760e-083]\n",
      " [3.10519230e-151]\n",
      " [3.54576893e-025]\n",
      " [7.58666536e-047]]\n",
      "3\n",
      "[[3.26499234e-142]\n",
      " [3.56052153e-040]\n",
      " [2.51776386e-004]\n",
      " [1.44883327e-001]\n",
      " [1.57244966e-117]\n",
      " [5.83689125e-112]\n",
      " [2.00943290e-083]\n",
      " [3.10533887e-151]\n",
      " [3.54579502e-025]\n",
      " [7.58677132e-047]]\n",
      "3\n",
      "[[3.26532092e-142]\n",
      " [3.56061220e-040]\n",
      " [2.51776312e-004]\n",
      " [1.44883216e-001]\n",
      " [1.57257892e-117]\n",
      " [5.83733283e-112]\n",
      " [2.00954149e-083]\n",
      " [3.10569019e-151]\n",
      " [3.54585754e-025]\n",
      " [7.58702530e-047]]\n",
      "3\n",
      "[[3.26487668e-142]\n",
      " [3.56048962e-040]\n",
      " [2.51776414e-004]\n",
      " [1.44883367e-001]\n",
      " [1.57240416e-117]\n",
      " [5.83673591e-112]\n",
      " [2.00939468e-083]\n",
      " [3.10521518e-151]\n",
      " [3.54577301e-025]\n",
      " [7.58668189e-047]]\n",
      "3\n",
      "[[3.26498779e-142]\n",
      " [3.56052028e-040]\n",
      " [2.51776387e-004]\n",
      " [1.44883329e-001]\n",
      " [1.57244787e-117]\n",
      " [5.83688513e-112]\n",
      " [2.00943139e-083]\n",
      " [3.10533402e-151]\n",
      " [3.54579415e-025]\n",
      " [7.58676781e-047]]\n",
      "3\n",
      "[[3.26500708e-142]\n",
      " [3.56052560e-040]\n",
      " [2.51776383e-004]\n",
      " [1.44883322e-001]\n",
      " [1.57245546e-117]\n",
      " [5.83691108e-112]\n",
      " [2.00943777e-083]\n",
      " [3.10535465e-151]\n",
      " [3.54579783e-025]\n",
      " [7.58678272e-047]]\n",
      "3\n",
      "[[3.26520891e-142]\n",
      " [3.56058130e-040]\n",
      " [2.51776339e-004]\n",
      " [1.44883254e-001]\n",
      " [1.57253486e-117]\n",
      " [5.83718243e-112]\n",
      " [2.00950449e-083]\n",
      " [3.10557039e-151]\n",
      " [3.54583623e-025]\n",
      " [7.58693869e-047]]\n",
      "3\n",
      "[[3.26492402e-142]\n",
      " [3.56050269e-040]\n",
      " [2.51776402e-004]\n",
      " [1.44883350e-001]\n",
      " [1.57242279e-117]\n",
      " [5.83679945e-112]\n",
      " [2.00941032e-083]\n",
      " [3.10526583e-151]\n",
      " [3.54578202e-025]\n",
      " [7.58671852e-047]]\n",
      "3\n",
      "[[3.26477574e-142]\n",
      " [3.56046177e-040]\n",
      " [2.51776437e-004]\n",
      " [1.44883401e-001]\n",
      " [1.57236445e-117]\n",
      " [5.83660027e-112]\n",
      " [2.00936132e-083]\n",
      " [3.10510725e-151]\n",
      " [3.54575380e-025]\n",
      " [7.58660386e-047]]\n",
      "3\n",
      "Accuracy of Model: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#IN ORDER TO RUN TRAINING, you'll need to download the .csv version of the MNIST dataset from \n",
    "#https://www.kaggle.com/datasets/oddrationale/mnist-in-csv and place mnist_train.csv and mnist_test.csv\n",
    "#in the mnist-dataset directory.\n",
    "\n",
    "class Neural_Network:\n",
    "    \n",
    "    def __init__(self, epochs, learningRate, batchSize):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.learningRate = learningRate\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "        #Reads in MNIST training set\n",
    "        self.data_initial = pd.read_csv('./mnist-dataset/mnist_train.csv')\n",
    "        self.labels = (self.data_initial['label']).to_numpy().reshape(60000,1) #(60000 imgs,)\n",
    "        self.data = (self.data_initial.drop('label', axis=1)).to_numpy().reshape(60000,784) #(60000 imgs, 784, pixels)\n",
    "        self.trainingSetSize = self.labels.size\n",
    "        \n",
    "        #Reads in MNIST test set\n",
    "        self.testInitial = pd.read_csv('./mnist-dataset/mnist_test.csv')\n",
    "        self.testLabels = (self.testInitial['label']).to_numpy().reshape(10000,1) #(10000 imgs,1)\n",
    "        self.testData = (self.testInitial.drop('label', axis=1)).to_numpy().reshape(10000,784) #(10000 imgs, 784, pixels)\n",
    "        self.testSetSize = self.testLabels.size\n",
    "\n",
    "        self.a_0 = np.zeros([784, 1]) #np.empty([10,1])\n",
    "\n",
    "        self.W_1 = np.random.default_rng().normal(loc=0, scale=(1/np.sqrt(16)), size=(16,784)) #Randomly intialized weight matrix\n",
    "        self.b_1 = np.random.default_rng().normal(loc=0, scale=1, size=(16,1)) #Randomly intialized bias column vector\n",
    "        self.z_1 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        self.a_1 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        self.error_1 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        \n",
    "        self.W_2 = np.random.default_rng().normal(loc=0, scale=(1/np.sqrt(16)), size=(16,16))\n",
    "        self.b_2 = np.random.default_rng().normal(loc=0, scale=1, size=(16,1))\n",
    "        self.z_2 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        self.a_2 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "        self.error_2 = np.zeros([16, 1]) #np.empty([16, 1])\n",
    "       \n",
    "        self.W_3 = np.random.default_rng().normal(loc=0, scale=(1/np.sqrt(10)), size=(10,16))\n",
    "        self.b_3 = np.random.default_rng().normal(loc=0, scale=1, size=(10,1))\n",
    "        self.z_3 = np.zeros([10, 1]) #np.empty([10, 1])\n",
    "        self.a_3 = np.zeros([10, 1]) #np.empty([10, 1])\n",
    "        self.error_out = np.zeros([10, 1]) #np.empty([10, 1])\n",
    "\n",
    "        self.dW_1 = np.zeros([16, 784]) # np.empty([16, 784])\n",
    "        self.dB_1 = np.zeros([16,1]) #np.empty([16,1])\n",
    "        self.dW_2 = np.zeros([16,16]) #np.empty([16,16])\n",
    "        self.dB_2 = np.zeros([16,1]) #np.empty([16,1])\n",
    "        self.dW_3 = np.zeros([10,16]) #np.empty([10,16])\n",
    "        self.dB_3 = np.zeros([10,1]) #np.empty([10,1])\n",
    "        \n",
    "        #Lesson: It seems np.empty can cause many errors.\n",
    "\n",
    "        self.y = np.zeros([10,1]).astype(int)\n",
    "\n",
    "    def sigmoid(self, colVector):\n",
    "        return 1/(1 + np.exp(-colVector))\n",
    "\n",
    "    def dSigmoid(self, colVector):\n",
    "        return (self.sigmoid(colVector)) * (1 - self.sigmoid(colVector))\n",
    "\n",
    "    def feedForward(self, x, dataset):\n",
    "        #Calculates all the activations in the network for the training example, x.\n",
    "\n",
    "        #Grabs image pixel information from the xth row of the dataset. \n",
    "        #This gives us a numpy (784,1) colm vector of activations for a training example, x, \n",
    "        #on Layer 0 (input layer)\n",
    "\n",
    "        #1 means feedForward a training example from the training dataset\n",
    "        if dataset == \"training\":\n",
    "            self.a_0 = self.sigmoid(self.data[x, :].reshape(784,1)) #MAKE SURE YOU SQUISH VALUES DOWN\n",
    "\n",
    "        #2 means feedForward a training example from the test dataset.\n",
    "        if dataset == \"testing\":\n",
    "            self.a_0 = self.sigmoid(self.testData[x, :].reshape(784,1)) #MAKE SURE YOU SQUISH VALUES DOWN\n",
    "\n",
    "        #Going into Layer 1\n",
    "        self.z_1 = (np.dot(self.W_1, self.a_0)) + self.b_1\n",
    "        self.a_1 = self.sigmoid(self.z_1)\n",
    "\n",
    "        #Going into Layer 2\n",
    "        self.z_2 = (np.dot(self.W_2, self.a_1)) + self.b_2\n",
    "        self.a_2 = self.sigmoid(self.z_2)\n",
    "\n",
    "        #Going into Layer 3 (output layer)\n",
    "        self.z_3 = (np.dot(self.W_3, self.a_2)) + self.b_3\n",
    "        self.a_3 = self.sigmoid(self.z_3)\n",
    "\n",
    "    def backProp(self, x): \n",
    "        #Calculates the \"error\" on all the neurons in the network for a training example, x.\n",
    "        \n",
    "        #Creates y column vector that represents the ideal output for all the output neurons for the spesific training example.\n",
    "        self.y[self.labels[x, 0], 0] = 1\n",
    "        #print(self.y)\n",
    "\n",
    "        #Calculuate the error on the output neurons\n",
    "        self.error_out = (self.a_3 - self.y) * self.dSigmoid(self.z_3)\n",
    "\n",
    "        #Calculate the error on each of neurons on each of the layers. Calculating backwards.\n",
    "        self.error_2 = np.dot((np.transpose(self.W_3)), self.error_out) * self.dSigmoid(self.z_2)\n",
    "        self.error_1 = np.dot((np.transpose(self.W_2)), self.error_2) * self.dSigmoid(self.z_1)\n",
    "\n",
    "        self.y = np.zeros([10,1]).astype(int)\n",
    "\n",
    "    def accumulateGradients(self):\n",
    "        #Calculates the derivative of the cost function WRT all the weights and biases. <-- Gradient information\n",
    "        #\"Accumulating\" (i.e. adding together) the graidents (element wise) of each of the training examples that go through.\n",
    "\n",
    "        self.dW_1 = self.dW_1 + (np.dot(self.error_1, np.transpose(self.a_0))) #(16,1) dot (1,784) = (16,784)\n",
    "        self.dB_1 = self.dB_1 + self.error_1 #(16,1)\n",
    "        \n",
    "        self.dW_2 = self.dW_2 + (np.dot(self.error_2, np.transpose(self.a_1))) #(16,1) dot (1,16) = (16,16)\n",
    "        self.dB_2 = self.dB_2 + self.error_2 #(16,1)\n",
    "\n",
    "        self.dW_3 = self.dW_3 + (np.dot(self.error_out, np.transpose(self.a_2))) #(10,1) dot (1,16) = (10,16)\n",
    "        self.dB_3 = self.dB_3 + self.error_out #(10,1)\n",
    "\n",
    "    def applyAvgGradient(self):\n",
    "        n = self.learningRate\n",
    "        m = self.batchSize\n",
    "        \n",
    "        self.W_1 = self.W_1 - ((n/m)*self.dW_1)\n",
    "        self.b_1 = self.b_1 - ((n/m)*self.dB_1)\n",
    "\n",
    "        self.W_2 = self.W_2 - ((n/m)*self.dW_2)\n",
    "        self.b_2 = self.b_2 - ((n/m)*self.dB_2)\n",
    "\n",
    "        self.W_3 = self.W_3 - ((n/m)*self.dW_3)\n",
    "        self.b_3 = self.b_3 - ((n/m)*self.dB_3)\n",
    "\n",
    "    def startTraining(self):\n",
    "        for epochs in range(self.epochs):\n",
    "\n",
    "            #60,000 examples (x), I want 1,000 examples per batch = 60 batch\n",
    "            for batch in range(int(self.trainingSetSize/self.batchSize)): #int(self.trainingSetSize/self.batchSize)\n",
    "                for x in range(self.batchSize):  #self.batchSize #1,000 per batch\n",
    "                    self.feedForward(x, \"training\")\n",
    "                    self.backProp(x)\n",
    "                    self.accumulateGradients()\n",
    "                self.applyAvgGradient() \n",
    "                #It seems the moment we apply the avgGradient, ALLL WEIGHT VALUES BECOME NAN\n",
    "\n",
    "            print(\"So far finished \", epochs, \" epoch\")\n",
    "     \n",
    "    def evaluate(self):\n",
    "        #Evaluates the models accuracy by running through the 10,000 test examples and seeing how many test examples the model gets right.\n",
    "        correct = 0\n",
    "        for x in range(10):\n",
    "            \n",
    "            self.feedForward(x, \"testing\")\n",
    "            print(self.a_3)\n",
    "            print(np.argmax(self.a_3))\n",
    "            \n",
    "            if (np.argmax(self.a_3) == self.testLabels[x, 0]):\n",
    "                correct += 1 \n",
    "        return (correct/10)*100 \n",
    "\n",
    "#12960 weights, 42 biases\n",
    "nn = Neural_Network(10, 0.1, 1000)\n",
    "nn.startTraining()\n",
    "print(\"Accuracy of Model:\", nn.evaluate(), \"%\")\n",
    "\n",
    "#Unknown Error Hypothesis:\n",
    "#For some reason, the model seems to refuse to update it's weights and biases, causing patterns in data to not be learned even after many epochs.\n",
    "#The activations on the last layer are changing slightly everytime we input a different test example, indicating we are recomputing a_3 for every x.\n",
    "#However, the activations in the last layer are so similar that it seems to me that the weights and biases are learning VERY VERY slowly.\n",
    "\n",
    "#The following was after a low learning rate (0.1) and 10 epochs\n",
    "#After the 10 epochs, I have noticed that the activation are incredibly small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf48f8f-6444-4e0f-966d-cf6161711931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
