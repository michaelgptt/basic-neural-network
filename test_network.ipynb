{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adf9e1d-2105-4c75-9d62-b9c7118f8dd5",
   "metadata": {},
   "source": [
    "### Experiment to Determine Which Values the Model is Getting Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f804bc7-c655-490e-8089-68abcc5b53d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far finished  0  epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tvm159\\AppData\\Local\\Temp\\2\\ipykernel_9756\\746534189.py:59: RuntimeWarning: overflow encountered in power\n",
      "  return 1/(1 + m.e**(-1*colVector))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far finished  1  epoch\n",
      "Accuracy of Model: 10.0 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "\n",
    "#IN ORDER TO RUN TRAINING, you'll need to download the .csv version of the MNIST dataset from \n",
    "#https://www.kaggle.com/datasets/oddrationale/mnist-in-csv and place mnist_train.csv and mnist_test.csv\n",
    "#in the mnist-dataset directory.\n",
    "\n",
    "class Neural_Network:\n",
    "    \n",
    "    def __init__(self, epochs, learningRate, batchSize):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.learningRate = learningRate\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "        #Reads in MNIST training set\n",
    "        self.data_initial = pd.read_csv('./mnist-dataset/mnist_train.csv')\n",
    "        self.labels = (self.data_initial['label']).to_numpy().reshape(60000,1) #(60000 imgs,)\n",
    "        self.data = (self.data_initial.drop('label', axis=1)).to_numpy().reshape(60000,784) #(60000 imgs, 784, pixels)\n",
    "        self.trainingSetSize = self.labels.size\n",
    "        \n",
    "        #Reads in MNIST test set\n",
    "        self.testInitial = pd.read_csv('./mnist-dataset/mnist_test.csv')\n",
    "        self.testLabels = (self.testInitial['label']).to_numpy().reshape(10000,1) #(10000 imgs,1)\n",
    "        self.testData = (self.testInitial.drop('label', axis=1)).to_numpy().reshape(10000,784) #(10000 imgs, 784, pixels)\n",
    "        self.testSetSize = self.testLabels.size\n",
    "\n",
    "        self.a_0 = np.empty([784, 1]) \n",
    "\n",
    "        self.W_1 = np.random.default_rng().normal(0, 1, size=(16,784)) #Randomly intialized weight matrix\n",
    "        self.b_1 = np.random.default_rng().normal(0, 1, size=(16,1)) #Randomly intialized bias column vector\n",
    "        self.z_1 = np.empty([16, 1])\n",
    "        self.a_1 = np.empty([16, 1])\n",
    "        self.error_1 = np.empty([16, 1])\n",
    "        \n",
    "        self.W_2 = np.random.default_rng().normal(0, 1, size=(16,16))\n",
    "        self.b_2 = np.random.default_rng().normal(0, 1, size=(16,1))\n",
    "        self.z_2 = np.empty([16, 1])\n",
    "        self.a_2 = np.empty([16, 1])\n",
    "        self.error_2 = np.empty([16, 1])\n",
    "       \n",
    "        self.W_3 = np.random.default_rng().normal(0, 1, size=(10,16))\n",
    "        self.b_3 = np.random.default_rng().normal(0, 1, size=(10,1))\n",
    "        self.z_3 = np.empty([10, 1])\n",
    "        self.a_3 = np.empty([10, 1])\n",
    "        self.error_out = np.empty([10, 1])\n",
    "\n",
    "        self.dW_1 = np.empty([16, 784])\n",
    "        self.dB_1 = np.empty([16,1])\n",
    "        self.dW_2 = np.empty([16,16])\n",
    "        self.dB_2 = np.empty([16,1])\n",
    "        self.dW_3 = np.empty([10,16])\n",
    "        self.dB_3 = np.empty([10,1])\n",
    "        \n",
    "        self.y = np.zeros([1,10]) \n",
    "\n",
    "    def sigmoid(self, colVector):\n",
    "        return 1/(1 + m.e**(-1*colVector))\n",
    "\n",
    "    def dSigmoid(self, colVector):\n",
    "        return (self.sigmoid(colVector)) * (1 - self.sigmoid(colVector))\n",
    "        \n",
    "    def feedForward(self, x, dataset):\n",
    "        #Calculates all the activations in the network for the training example, x.\n",
    "\n",
    "        #Grabs image pixel information from the xth row of the dataset. \n",
    "        #This gives us a numpy (784,1) colm vector of activations for a training example, x, \n",
    "        #on Layer 0 (input layer)\n",
    "\n",
    "        #1 means feedForward a training example from the training dataset\n",
    "        if dataset == 1:\n",
    "            self.a_0 = self.data[0, :].reshape(784,1)\n",
    "\n",
    "        #2 means feedForward a training example from the test dataset.\n",
    "        if dataset == 2:\n",
    "            self.a_0 = self.testData[0, :].reshape(784,1)\n",
    "\n",
    "        #Going into Layer 1\n",
    "        self.z_1 = (np.dot(self.W_1, self.a_0)) + self.b_1\n",
    "        self.a_1 = self.sigmoid(self.z_1)\n",
    "\n",
    "        #Going into Layer 2\n",
    "        self.z_2 = (np.dot(self.W_2, self.a_1)) + self.b_2\n",
    "        self.a_2 = self.sigmoid(self.z_2)\n",
    "\n",
    "        #Going into Layer 3 (output layer)\n",
    "        self.z_3 = (np.dot(self.W_3, self.a_2)) + self.b_3\n",
    "        self.a_3 = self.sigmoid(self.z_3)\n",
    "        \n",
    "#    \n",
    "    def backProp(self, x): \n",
    "        #Calculates the \"error\" on all the neurons in the network for a training example, x.\n",
    "        \n",
    "        #Creates the y column vector that represents the ideal output for all the output neurons for the spesific training example.\n",
    "        label = self.labels[x, 0]\n",
    "        self.y[0,label] = label\n",
    "        self.y = np.transpose(self.y)\n",
    "\n",
    "\n",
    "        #********************************************************************************************\n",
    "        #Calculuate the error on the output neurons\n",
    "        self.error_out = (self.a_3 - self.y) * self.dSigmoid(self.z_3)\n",
    "\n",
    "        #Calculate the error on each of neurons on each of the layers. Calculating backwards.\n",
    "        self.error_2 = np.dot((np.transpose(self.W_3)), self.error_out) * self.dSigmoid(self.z_2)\n",
    "        self.error_1 = np.dot((np.transpose(self.W_2)), self.error_2) * self.dSigmoid(self.z_1)\n",
    "\n",
    "        self.y = np.zeros([1,10])\n",
    "\n",
    "    def accumulateGradients(self):\n",
    "        #Calculates the derivative of the cost function WRT all the weights and biases. <-- Gradient information\n",
    "        #\"Accumulating\" (i.e. adding together) the graidents (element wise) of each of the training examples that go through.\n",
    "\n",
    "        self.dW_1 += (np.dot(self.error_1, np.transpose(self.a_0))) #(16,1) dot (1,784) = (16,784)\n",
    "        self.dB_1 += self.error_1 #(16,1)\n",
    "        \n",
    "        self.dW_2 += (np.dot(self.error_2, np.transpose(self.a_1))) #(16,1) dot (1,16) = (16,16)\n",
    "        self.dB_2 += self.error_2 #(16,1)\n",
    "\n",
    "        self.dW_3 += (np.dot(self.error_out, np.transpose(self.a_2))) #(10,1) dot (1,16) = (10,16)\n",
    "        self.dB_3 += self.error_out #(10,1)\n",
    "\n",
    "    def applyAvgGradient(self):\n",
    "        n = self.learningRate\n",
    "        m = self.batchSize\n",
    "        \n",
    "        self.W_1 -= ((n/m)*self.dW_1)\n",
    "        self.b_1 -= ((n/m)*self.dB_1)\n",
    "\n",
    "        self.W_2 -= ((n/m)*self.dW_2)\n",
    "        self.b_2 -= ((n/m)*self.dB_2)\n",
    "\n",
    "        self.W_3 -= ((n/m)*self.dW_3)\n",
    "        self.b_3 -= ((n/m)*self.dB_3)\n",
    "\n",
    "    def startTraining(self):\n",
    "        for epochs in range(self.epochs):\n",
    "            print(\"So far finished \", epochs, \" epoch\")\n",
    "\n",
    "            #60,000 examples (x), I want 1,000 examples per batch = 60 batch\n",
    "            for batch in range(int(self.trainingSetSize/self.batchSize)):\n",
    "                for x in range(self.batchSize): #1,000 per batch\n",
    "                    self.feedForward(x, 1)\n",
    "                    self.backProp(x)\n",
    "                    self.accumulateGradients()\n",
    "                self.applyAvgGradient()\n",
    "                \n",
    "    def evaluate(self):\n",
    "        #Evaluates the models accuracy by running through the 10,000 test examples and seeing how many test examples the model gets right.\n",
    "        \n",
    "        correct = 0\n",
    "        for x in range(10): #self.testSetSize\n",
    "            \n",
    "            testLabel = self.testLabels[x, 0] #Must return a scalar of the activation\n",
    "            \n",
    "            self.feedForward(x, 2) #Finds new a_3 \n",
    "            \n",
    "            #Determine which activation is biggest in final layer. I.e. which neuron is the model's choice?\n",
    "            ############################################################################################### BEWARE ###################################\n",
    "            \n",
    "            # Okay so. For some reason, when we call self.a_3 THE FIRST TIME, i.e. assign it to a variable, etc. THE IF STATEMENT GETS EVALUATED.\n",
    "            # Run the model a 2ND TIME, the if statement no long calls.\n",
    "            # BOTH TIMES, the model accuracy is the same. Namely, when the print statement actually works\n",
    "            # It shows that the the index of the column with the greatest activations on it in a_3 always keeps going up to like 7, 8 or 9.\n",
    "            \n",
    "            biggestActivation = 0\n",
    "            theIndexOfBiggest = 0\n",
    "            \n",
    "            for i in range(self.a_3.size):\n",
    "                if (self.a_3[i,0] > biggestActivation): #Never Gets evaluated because self.a_3 is full of NaNs\n",
    "                    biggestActivation = self.a_3[i,0]\n",
    "                    theIndexOfBiggest = i                   \n",
    "\n",
    "                    print(\"testLabel: \",testLabel,\" vs indexOfBiggest\",theIndexOfBiggest)\n",
    "                    \n",
    "            if (theIndexOfBiggest == testLabel):\n",
    "                correct = correct + 1\n",
    "                #print(\"testLabel: \",testLabel,\" vs indexOfBiggest\",theIndexOfBiggest)\n",
    "                \n",
    "        return (correct/10)*100 #self.testSetSize\n",
    "        \n",
    "#12960 weights, 42 biases\n",
    "nn = Neural_Network(2, 0.1, 1000)\n",
    "nn.startTraining()\n",
    "print(\"Accuracy of Model:\", nn.evaluate(), \"%\")\n",
    "\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# #I've noticed that whether after 5 epochs, 1 epoch, 0.1 LR or 1 LR, epoch, each time with random weights and biases, we keep on \"converging\" onto a single value of 9.8%\n",
    "\n",
    "# X1\n",
    "# #Interestingly, after testing with only 100 test cases, we are CONSISTENTLY getting ONLY the number 0 correct with every run of training --> then evaluating.\n",
    "# There was one case where we were CONSISTENTLY ONLY getting the number 9 correct as well.\n",
    "\n",
    "# What we can say:\n",
    "# The model does seem to be converging the randomly intialized weights and biases into something... \"normalized\"\n",
    "# Why it's ONLY being incentivised to classify things as a 0 is currently unknown.\n",
    "\n",
    "# Next X\n",
    "# The first 15 rows of mnist_test.csv has 3 labels with 0. Therefore, if we run the first 15 examples in mnist_test.csv, we should get a 20% accuracy.\n",
    "\n",
    "# X2:\n",
    "# My hypothesis from X1 was correct, we got a 20% training accuracy. Therefore, it seems the model is being incentivised to ONLY classify things as zero, everything else:\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a902b-77a5-4370-bee6-3e68cfde20a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
