{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf3c9172-0b0e-4962-8356-535bc315fc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0\n",
      "Finished epoch 1\n",
      "Finished epoch 2\n",
      "Finished epoch 3\n",
      "Finished epoch 4\n",
      "Finished epoch 5\n",
      "Finished epoch 6\n",
      "Finished epoch 7\n",
      "Finished epoch 8\n",
      "Finished epoch 9\n",
      "Accuracy of Model: 93.27 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Neural_Network:\n",
    "    \n",
    "    def __init__(self, epochs, learningRate, batchSize):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.learningRate = learningRate\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "        #Reads in MNIST training set\n",
    "        self.data_initial = pd.read_csv('./mnist-dataset/mnist_train.csv')\n",
    "        self.labels = (self.data_initial['label']).to_numpy().reshape(60000,1)\n",
    "        \n",
    "        # --- FIX 1: NORMALIZE DATA ---\n",
    "        # Divide by 255.0 so inputs are between 0 and 1. \n",
    "        # This prevents the neurons from saturating immediately.\n",
    "        self.data = (self.data_initial.drop('label', axis=1)).to_numpy().reshape(60000,784) / 255.0\n",
    "        self.trainingSetSize = self.labels.size\n",
    "        \n",
    "        #Reads in MNIST test set\n",
    "        self.testInitial = pd.read_csv('./mnist-dataset/mnist_test.csv')\n",
    "        self.testLabels = (self.testInitial['label']).to_numpy().reshape(10000,1)\n",
    "        # --- FIX 1: NORMALIZE TEST DATA TOO ---\n",
    "        self.testData = (self.testInitial.drop('label', axis=1)).to_numpy().reshape(10000,784) / 255.0\n",
    "        self.testSetSize = self.testLabels.size\n",
    "\n",
    "        self.a_0 = np.zeros([784, 1])\n",
    "\n",
    "        # Xavier Initialization is fine here\n",
    "        self.W_1 = np.random.default_rng().normal(loc=0, scale=(1/np.sqrt(784)), size=(16,784)) \n",
    "        self.b_1 = np.random.default_rng().normal(loc=0, scale=1, size=(16,1))\n",
    "        self.z_1 = np.zeros([16, 1])\n",
    "        self.a_1 = np.zeros([16, 1])\n",
    "        self.error_1 = np.zeros([16, 1])\n",
    "        \n",
    "        self.W_2 = np.random.default_rng().normal(loc=0, scale=(1/np.sqrt(16)), size=(16,16))\n",
    "        self.b_2 = np.random.default_rng().normal(loc=0, scale=1, size=(16,1))\n",
    "        self.z_2 = np.zeros([16, 1])\n",
    "        self.a_2 = np.zeros([16, 1])\n",
    "        self.error_2 = np.zeros([16, 1])\n",
    "        \n",
    "        self.W_3 = np.random.default_rng().normal(loc=0, scale=(1/np.sqrt(16)), size=(10,16))\n",
    "        self.b_3 = np.random.default_rng().normal(loc=0, scale=1, size=(10,1))\n",
    "        self.z_3 = np.zeros([10, 1])\n",
    "        self.a_3 = np.zeros([10, 1])\n",
    "        self.error_out = np.zeros([10, 1])\n",
    "\n",
    "        self.dW_1 = np.zeros([16, 784])\n",
    "        self.dB_1 = np.zeros([16,1])\n",
    "        self.dW_2 = np.zeros([16,16])\n",
    "        self.dB_2 = np.zeros([16,1])\n",
    "        self.dW_3 = np.zeros([10,16])\n",
    "        self.dB_3 = np.zeros([10,1])\n",
    "        \n",
    "        self.y = np.zeros([10,1]).astype(int)\n",
    "\n",
    "    def sigmoid(self, colVector):\n",
    "        # Clip values to prevent overflow in exp\n",
    "        colVector = np.clip(colVector, -500, 500)\n",
    "        return 1/(1 + np.exp(-colVector))\n",
    "\n",
    "    def dSigmoid(self, colVector):\n",
    "        return (self.sigmoid(colVector)) * (1 - self.sigmoid(colVector))\n",
    "\n",
    "    def feedForward(self, x, dataset):\n",
    "        if dataset == \"training\":\n",
    "            # --- FIX 2: DO NOT SIGMOID INPUT ---\n",
    "            # The data is already normalized 0-1. Just take it as is.\n",
    "            self.a_0 = self.data[x, :].reshape(784,1)\n",
    "\n",
    "        if dataset == \"testing\":\n",
    "            # --- FIX 2: DO NOT SIGMOID INPUT ---\n",
    "            self.a_0 = self.testData[x, :].reshape(784,1)\n",
    "\n",
    "        #Going into Layer 1\n",
    "        self.z_1 = (np.dot(self.W_1, self.a_0)) + self.b_1\n",
    "        self.a_1 = self.sigmoid(self.z_1)\n",
    "\n",
    "        #Going into Layer 2\n",
    "        self.z_2 = (np.dot(self.W_2, self.a_1)) + self.b_2\n",
    "        self.a_2 = self.sigmoid(self.z_2)\n",
    "\n",
    "        #Going into Layer 3 (output layer)\n",
    "        self.z_3 = (np.dot(self.W_3, self.a_2)) + self.b_3\n",
    "        self.a_3 = self.sigmoid(self.z_3)\n",
    "\n",
    "    def backProp(self, x): \n",
    "        self.y[self.labels[x, 0], 0] = 1\n",
    "        \n",
    "        self.error_out = (self.a_3 - self.y) * self.dSigmoid(self.z_3)\n",
    "\n",
    "        self.error_2 = np.dot((np.transpose(self.W_3)), self.error_out) * self.dSigmoid(self.z_2)\n",
    "        self.error_1 = np.dot((np.transpose(self.W_2)), self.error_2) * self.dSigmoid(self.z_1)\n",
    "\n",
    "        self.y = np.zeros([10,1]).astype(int)\n",
    "\n",
    "    def accumulateGradients(self):\n",
    "        self.dW_1 += (np.dot(self.error_1, np.transpose(self.a_0)))\n",
    "        self.dB_1 += self.error_1\n",
    "        \n",
    "        self.dW_2 += (np.dot(self.error_2, np.transpose(self.a_1)))\n",
    "        self.dB_2 += self.error_2\n",
    "\n",
    "        self.dW_3 += (np.dot(self.error_out, np.transpose(self.a_2)))\n",
    "        self.dB_3 += self.error_out\n",
    "\n",
    "    def applyAvgGradient(self):\n",
    "        n = self.learningRate\n",
    "        m = self.batchSize\n",
    "        \n",
    "        self.W_1 -= ((n/m)*self.dW_1)\n",
    "        self.b_1 -= ((n/m)*self.dB_1)\n",
    "\n",
    "        self.W_2 -= ((n/m)*self.dW_2)\n",
    "        self.b_2 -= ((n/m)*self.dB_2)\n",
    "\n",
    "        self.W_3 -= ((n/m)*self.dW_3)\n",
    "        self.b_3 -= ((n/m)*self.dB_3)\n",
    "\n",
    "        # --- FIX 3: RESET GRADIENTS ---\n",
    "        # If you don't do this, gradients pile up forever -> infinity -> NaN\n",
    "        self.dW_1.fill(0)\n",
    "        self.dB_1.fill(0)\n",
    "        self.dW_2.fill(0)\n",
    "        self.dB_2.fill(0)\n",
    "        self.dW_3.fill(0)\n",
    "        self.dB_3.fill(0)\n",
    "\n",
    "    def startTraining(self):\n",
    "        for epochs in range(self.epochs):\n",
    "            for batch in range(int(self.trainingSetSize/self.batchSize)): \n",
    "                for x in range(self.batchSize):\n",
    "                    \n",
    "                    # --- FIX 4: CORRECT INDEXING ---\n",
    "                    # Calculate the actual index in the dataset\n",
    "                    data_index = (batch * self.batchSize) + x\n",
    "                    \n",
    "                    self.feedForward(data_index, \"training\")\n",
    "                    self.backProp(data_index)\n",
    "                    self.accumulateGradients()\n",
    "                    \n",
    "                self.applyAvgGradient() \n",
    "            \n",
    "            # Print accuracy on a small subset to check progress\n",
    "            print(f\"Finished epoch {epochs}\")\n",
    "     \n",
    "    def evaluate(self):\n",
    "        correct = 0\n",
    "        for x in range(10000): \n",
    "            self.feedForward(x, \"testing\")\n",
    "            if (np.argmax(self.a_3) == self.testLabels[x, 0]):\n",
    "                correct += 1 \n",
    "        return (correct/10000)*100 \n",
    "\n",
    "# Run it\n",
    "nn = Neural_Network(epochs=10, learningRate=0.5, batchSize=1) # Increased LR, decreased Batch size for faster updates\n",
    "nn.startTraining()\n",
    "print(\"Accuracy of Model:\", nn.evaluate(), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f98f8-11db-4c69-9794-b4ae1adbcdaa",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "Very frustrating but still very fun project. There are still many things I am trying to learn more about regarding this network. For instance, why is it that when I increase the batch size, the accuracy seems to decrease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd37ca-5bc0-4402-a4b2-14a4c0c4e97a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
